{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e217f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from config.config_continuous import config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04288aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2_mean = nn.Linear(hidden_size, output_size)  # Pour la moyenne\n",
    "        self.fc2_logstd = nn.Linear(hidden_size, output_size)  # Pour la log-variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        log_std = self.fc2_logstd(x)\n",
    "        return mean, log_std  # Retourne la moyenne et la log-variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c248b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ContinuousPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        actor_learning_rate,\n",
    "        critic_learning_rate,\n",
    "        lambda_=0.95,\n",
    "        writer=None,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 0.2\n",
    "        self.writer = SummaryWriter( log_dir=\"runs/ppo_continuous_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "        # Reset\n",
    "        hidden_size = 64\n",
    "        \n",
    "        obs_size = self.observation_space.shape[0] * self.observation_space.shape[1] * self.observation_space.shape[2]\n",
    "        action_dim = self.action_space.shape[0]\n",
    "\n",
    "\n",
    "        self.actor = Net(obs_size, hidden_size, action_dim)\n",
    "        self.critic = Net(obs_size, hidden_size, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.actor.parameters(), lr=self.actor_learning_rate\n",
    "        )\n",
    "\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            params=self.critic.parameters(), lr=self.critic_learning_rate\n",
    "        )\n",
    "        self.current_episode = []\n",
    "        self.episode_reward = 0\n",
    "\n",
    "        self.scores = []\n",
    "\n",
    "        self.n_eps = 0\n",
    "        self.total_steps = 0\n",
    "        self.critic_updates = 0\n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \n",
    "        state_tensor = torch.tensor(state.flatten()).unsqueeze(0).float()\n",
    "        with torch.no_grad():\n",
    "            mean, log_std = self.actor(state_tensor)  # Récupérer la moyenne et la log-variance\n",
    "            std = torch.exp(log_std)  # L'écart-type est l'exponentielle de la log-variance\n",
    "            dist = torch.distributions.Normal(mean, std)\n",
    "            action = dist.sample()  # Échantillonner une action à partir de la distribution\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)  # Calculer la log-probabilité\n",
    "        return action, log_prob\n",
    "\n",
    "    def compute_GAE(self, rewards, terminateds, advantages):\n",
    "        \"\"\"\n",
    "        Generalized Advantage Estimation\n",
    "        \"\"\"\n",
    "        GAE = 0\n",
    "        GAE_list = []\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            GAE = (1 - terminateds[t]) * GAE\n",
    "            GAE = advantages[t] + self.gamma * self.lambda_ * GAE\n",
    "            GAE_list.append(GAE)\n",
    "        return torch.tensor(GAE_list[::-1], dtype=torch.float32)\n",
    "\n",
    "    def compute_ppo_score(self):\n",
    "        states, actions, rewards, terminals, next_states, old_log_probs = tuple(\n",
    "            [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "        )\n",
    "        \n",
    "        next_states = next_states.view(next_states.size(0), -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.critic(next_states)\n",
    "            while isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "\n",
    "            target_values = (\n",
    "                rewards\n",
    "                + self.gamma * (1 - terminals) * output.squeeze()\n",
    "            )\n",
    "            states = states.view(states.size(0), -1)\n",
    "            values = self.critic(states)\n",
    "            while isinstance(values, tuple):\n",
    "                values = values[0]\n",
    "            values = values.squeeze()\n",
    "            advantages = target_values - values\n",
    "\n",
    "        GAEs = self.compute_GAE(rewards, terminals, advantages)\n",
    "        std = GAEs.std()\n",
    "        GAEs = (GAEs - GAEs.mean()) / ( std + 1e-8 )\n",
    "\n",
    "        # Calcul des log-probabilités pour les actions continues\n",
    "        mean, log_std = self.actor(states)\n",
    "        std = torch.exp(log_std)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.eps, 1 + self.eps)\n",
    "        policy_loss = -torch.min(ratio * GAEs, clipped_ratio * GAEs).mean()\n",
    "        \n",
    "\n",
    "        # Perte de valeur (Critic loss)\n",
    "        value_loss = self.loss_function(values, target_values)\n",
    "        \n",
    "        # Calcul de l'entropie pour les actions continues\n",
    "        entropy = 0.5 + 0.5 * np.log(2 * np.pi) + log_std.mean()\n",
    "        entropy_loss = -0.01 * entropy  # Coefficient d'entropie\n",
    "\n",
    "        # Perte totale\n",
    "        total_loss = policy_loss + 0.5 * value_loss + entropy_loss\n",
    "        if self.writer:\n",
    "            probs = torch.softmax(mean, dim=-1)\n",
    "\n",
    "            entropy = -(probs * torch.log_softmax(mean, dim=-1)).sum(dim=1).mean()\n",
    "            self.writer.add_scalar(\"policy/entropy\", entropy.item(), self.n_eps)\n",
    "\n",
    "        return total_loss.sum().unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    def train_reset(self):\n",
    "        self.current_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.scores = []\n",
    "\n",
    "    def update_critic(self, transition):\n",
    "        state, _, reward, terminated, next_state, _ = transition\n",
    "        \n",
    "        values = self.critic.forward(state)\n",
    "        next_state = next_state.flatten()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_values = (1 - terminated) * self.critic(next_state)[0]\n",
    "            \n",
    "            targets =  reward + next_state_values * self.gamma\n",
    "        \n",
    "        loss = self.loss_function(values[0].float(), targets.unsqueeze(1).float())\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"loss/critic\", loss.item(), self.total_steps)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(self.critic.parameters(), 5)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        state_tensor = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mean, log_std = self.actor(state_tensor)\n",
    "            std = torch.exp(log_std)\n",
    "            dist = torch.distributions.Normal(mean, std)\n",
    "            old_log_probs = dist.log_prob(action_tensor).sum(dim=-1)\n",
    "            \n",
    "        transition = (\n",
    "            state_tensor,\n",
    "            action_tensor,\n",
    "            torch.tensor([reward], dtype=torch.float32),\n",
    "            torch.tensor([terminated], dtype=torch.float32),\n",
    "            torch.tensor(next_state.flatten(), dtype=torch.float32).unsqueeze(0),\n",
    "            old_log_probs,\n",
    "        )\n",
    "\n",
    "        self.total_steps += 1\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        self.current_episode.append(transition)\n",
    "        self.update_critic(transition)\n",
    "\n",
    "        if terminated:\n",
    "            self.writer.add_scalar(\"policy/reward\", self.episode_reward, self.n_eps)\n",
    "            self.episode_reward = 0\n",
    "            self.n_eps += 1\n",
    "            \n",
    "            # Mise à jour de l'acteur à la fin de chaque épisode au lieu d'attendre batch_size épisodes\n",
    "            if len(self.current_episode) > 0:\n",
    "                # Mise à jour de l'acteur\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.compute_ppo_score()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.writer:\n",
    "                    self.writer.add_scalar(\"loss/total\", loss.item(), self.n_eps)\n",
    "                \n",
    "                self.current_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebaff869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, *args):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, *data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e92d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env, n_sim=5):\n",
    "    \"\"\"\n",
    "    ** Solution **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action_tensor = agent.get_action(state, 0)[0]  # Extrait le premier élément du tuple\n",
    "            \n",
    "            action = action_tensor.squeeze(0)\n",
    "            action = action.numpy()\n",
    "            state, reward, terminated, truncated, info= env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            on_road = info[\"rewards\"].get(\"on_road_reward\", True)  # Default to True if key is missing\n",
    "            done = done or truncated or (not on_road)\n",
    "            done = terminated or truncated or (not on_road)\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb6dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m         display_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mrun_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#print(f'Average over 5 runs : {np.mean(eval_agent(agent, env))}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mrun_one_episode\u001b[0;34m(env, agent, display)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action_tensor)\n\u001b[0;32m---> 21\u001b[0m state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43mdisplay_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m on_road \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_road_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Default to True if key is missing\u001b[39;00m\n\u001b[1;32m     23\u001b[0m done \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m on_road)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/highway_env/envs/common/abstract.py:269\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames):\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# Forward action to the vehicle\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    260\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    268\u001b[0m     ):\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/highway_env/envs/common/action.py:161\u001b[0m, in \u001b[0;36mContinuousAction.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m action\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/highway_env/envs/common/action.py:147\u001b[0m, in \u001b[0;36mContinuousAction.get_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mMIN_SPEED,\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolled_vehicle\u001b[38;5;241m.\u001b[39mMAX_SPEED,\n\u001b[1;32m    143\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed_range\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongitudinal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlateral:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(action[\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration_range),\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_range),\n\u001b[1;32m    148\u001b[0m     }\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongitudinal:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m: utils\u001b[38;5;241m.\u001b[39mlmap(action[\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration_range),\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    153\u001b[0m     }\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import highway_env\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode='rgb_array')\n",
    "env.unwrapped.configure(config_dict)\n",
    "obs, info = env.reset()\n",
    "agent = RandomAgent(env.observation_space, env.action_space)\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action_tensor = agent.get_action(state)  # Extrait le premier élément du tuple\n",
    "        try:\n",
    "            action = np.array(action_tensor[0])\n",
    "        except:\n",
    "            action = np.array(action_tensor)\n",
    "        state, reward, done, truncated, info = display_env.step(action.numpy())\n",
    "        on_road = info[\"rewards\"].get(\"on_road_reward\", True)  # Default to True if key is missing\n",
    "        done = done or truncated or (not on_road)\n",
    "        print(f\"done {done} truncated {truncated}\")\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')\n",
    "    \n",
    "run_one_episode(env, agent, display=True)\n",
    "#print(f'Average over 5 runs : {np.mean(eval_agent(agent, env))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 10 , reward =  7.621009905437174\n",
      "episode = 20 , reward =  7.956120064045619\n",
      "episode = 30 , reward =  5.21272981996118\n",
      "episode = 40 , reward =  6.298177101117256\n",
      "episode = 50 , reward =  8.830050724533308\n",
      "episode = 60 , reward =  6.826909720281729\n",
      "episode = 70 , reward =  6.116939001679404\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "    \n",
    "env = gym.make(\"racetrack-v0\", render_mode='rgb_array')\n",
    "env.unwrapped.config.update(config_dict)\n",
    "obs, info = env.reset()\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = 0.95\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "actor_lr = 5e-4\n",
    "critic_lr = 1e-3\n",
    "\n",
    "arguments = (\n",
    "    action_space,\n",
    "    observation_space,\n",
    "    gamma,\n",
    "    batch_size,              # rename for clarity if needed\n",
    "    actor_lr,\n",
    "    critic_lr\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_episodes = 500\n",
    "\n",
    "agent = ContinuousPPO(*arguments)\n",
    "\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            state.flatten()\n",
    "            action_tensor = agent.get_action(state)[0]  # Extrait le premier élément du tuple\n",
    "            action = action_tensor.squeeze(0)   # Enlève la première dimension (batch) et convertit en np.array\n",
    "\n",
    "            action = np.array(action)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            on_road = info[\"rewards\"].get(\"on_road_reward\", True)  # Default to True if key is missing\n",
    "            done = terminated or truncated or (not on_road)\n",
    "                                            \n",
    "            loss_val = agent.update(state, action, reward, done, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "            \n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses\n",
    "\n",
    "    \n",
    "# Run the training loop\n",
    "losses = train(env, agent, N_episodes)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2458471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length 316.4605487862365\n"
     ]
    }
   ],
   "source": [
    "run_one_episode(env, agent, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetContinousActions(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_size, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, act_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(\n",
    "            torch.zeros(act_dim)\n",
    "        )  # learnable log std for Gaussian policy\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean = self.net(obs)\n",
    "        std = self.log_std.exp()\n",
    "        return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e408ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEContinuous:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        gamma,\n",
    "        episode_batch_size,\n",
    "        learning_rate,\n",
    "        writer=None,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.episode_batch_size = episode_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.writer = writer\n",
    "\n",
    "        # Reset\n",
    "        hidden_size = 128\n",
    "\n",
    "        obs_size = self.observation_space.shape[0] * self.observation_space.shape[1] * self.observation_space.shape[2]\n",
    "        actions_dim = self.action_space.shape[0]\n",
    "\n",
    "        self.policy_net = NetContinousActions(obs_size, hidden_size, actions_dim)\n",
    "\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        self.n_eps = 0\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        self.current_episode.append(\n",
    "            (\n",
    "                torch.tensor(state).unsqueeze(0),\n",
    "                torch.tensor(action, dtype=torch.float32).unsqueeze(0),\n",
    "                torch.tensor(reward, dtype=torch.float32).unsqueeze(0),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if terminated:\n",
    "            self.n_eps += 1\n",
    "\n",
    "            states, actions, rewards = tuple(\n",
    "                [torch.cat(data) for data in zip(*self.current_episode)]\n",
    "            )\n",
    "\n",
    "            current_episode_returns = self._returns(rewards, self.gamma)\n",
    "            current_episode_returns = (\n",
    "                current_episode_returns - current_episode_returns.mean()\n",
    "            ) / current_episode_returns.std()\n",
    "            # discounts = torch.pow(self.gamma, torch.arange(len(rewards)))\n",
    "            states = states.view(states.size(0), -1)\n",
    "            means, stds = self.policy_net.forward(states)\n",
    "            dist = torch.distributions.Normal(means, stds)\n",
    "            log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "\n",
    "            score = log_probs * current_episode_returns  # * discounts\n",
    "\n",
    "            self.scores.append(score.sum().unsqueeze(0))\n",
    "            self.current_episode = []\n",
    "\n",
    "\n",
    "            if (self.n_eps % self.episode_batch_size) == 0:\n",
    "                self.optimizer.zero_grad()\n",
    "                full_neg_score = -torch.cat(self.scores).sum() / self.episode_batch_size\n",
    "                full_neg_score.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.policy_net.parameters(), max_norm=1.0\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "\n",
    "                \n",
    "                self.scores = []\n",
    "\n",
    "        return\n",
    "\n",
    "    def _returns(self, rewards, gamma):\n",
    "        \"\"\" \"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\" \"\"\"\n",
    "        state_tensor = (\n",
    "            torch.as_tensor(state, dtype=torch.float32).flatten().unsqueeze(0)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.policy_net(state_tensor)\n",
    "            action = torch.distributions.Normal(mean, std).sample()\n",
    "            low = torch.tensor(self.action_space.low)\n",
    "            high = torch.tensor(self.action_space.high)\n",
    "            return action.clamp(low, high).numpy()[0]\n",
    "\n",
    "    def train_reset(self):\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "\n",
    "        obs_size = self.observation_space.shape[0]\n",
    "        actions_dim = self.action_space.shape[0]\n",
    "\n",
    "        self.policy_net = NetContinousActions(obs_size, hidden_size, actions_dim)\n",
    "\n",
    "        self.scores = []\n",
    "        self.current_episode = []\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.policy_net.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "        self.n_eps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eff891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 10 , reward =  8.963257965347832\n",
      "episode = 20 , reward =  6.178666309653676\n",
      "episode = 30 , reward =  6.903009863944772\n",
      "episode = 40 , reward =  5.978077839028659\n",
      "episode = 50 , reward =  5.21868445509112\n",
      "episode = 60 , reward =  10.63696340647139\n",
      "episode = 70 , reward =  8.002335558514755\n",
      "episode = 80 , reward =  14.387552407856328\n",
      "episode = 90 , reward =  11.3025331775272\n",
      "episode = 100 , reward =  24.07472614286081\n",
      "episode = 110 , reward =  18.26818283703865\n",
      "episode = 120 , reward =  5.7733257388073325\n",
      "episode = 130 , reward =  6.903622368929507\n",
      "episode = 140 , reward =  9.688435128342016\n",
      "episode = 150 , reward =  19.957281082267475\n",
      "episode = 160 , reward =  17.084277668122493\n",
      "episode = 170 , reward =  13.846186599220484\n",
      "episode = 180 , reward =  12.506736142436123\n",
      "episode = 190 , reward =  17.423673406032215\n",
      "episode = 200 , reward =  14.54028883866198\n",
      "episode = 210 , reward =  28.427730411931634\n",
      "episode = 220 , reward =  16.172523169320186\n",
      "episode = 230 , reward =  36.13937229028787\n",
      "episode = 240 , reward =  39.19926567768057\n",
      "episode = 250 , reward =  37.42083101257741\n",
      "episode = 260 , reward =  43.04576738503316\n",
      "episode = 270 , reward =  15.7380617892487\n",
      "episode = 280 , reward =  48.473351699245825\n",
      "episode = 290 , reward =  50.02903826477249\n",
      "episode = 300 , reward =  49.65005235571829\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "    \n",
    "env = gym.make(\"racetrack-v0\", render_mode='rgb_array')\n",
    "env.unwrapped.config.update(config_dict)\n",
    "obs, info = env.reset()\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = 0.95\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "actor_lr = 5e-4\n",
    "critic_lr = 1e-3\n",
    "\n",
    "arguments = (\n",
    "    action_space,\n",
    "    observation_space,\n",
    "    gamma,\n",
    "    batch_size,              # rename for clarity if needed\n",
    "    actor_lr,\n",
    "    critic_lr\n",
    ")\n",
    "\n",
    "agent = REINFORCEContinuous(*arguments)\n",
    "\n",
    "\n",
    "\n",
    "N_episodes = 500\n",
    "\n",
    "#agent = ContinuousPPO(*arguments)\n",
    "\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            state.flatten()\n",
    "            action_tensor = agent.get_action(state)[0]  # Extrait le premier élément du tuple\n",
    "            action = action_tensor.squeeze(0)   # Enlève la première dimension (batch) et convertit en np.array\n",
    "\n",
    "            action = np.array([action])\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            on_road = info[\"rewards\"].get(\"on_road_reward\", True)\n",
    "            done = terminated or truncated or (not on_road)\n",
    "            loss_val = agent.update(state, action, reward, done, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            \n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses\n",
    "\n",
    "    \n",
    "# Run the training loop\n",
    "losses = train(env, agent, N_episodes)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fb488",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_one_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_one_episode\u001b[49m(env, agent, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_one_episode' is not defined"
     ]
    }
   ],
   "source": [
    "run_one_episode(env, agent, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
