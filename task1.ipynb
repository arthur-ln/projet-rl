{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "#replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN: \n",
    "    def __init__(self,\n",
    "                action_space,\n",
    "                observation_space,\n",
    "                gamma,\n",
    "                batch_size,\n",
    "                buffer_capacity,\n",
    "                update_target_every, \n",
    "                epsilon_start, \n",
    "                decrease_epsilon_factor, \n",
    "                epsilon_min,\n",
    "                learning_rate,\n",
    "                ): \n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.update_target_every = update_target_every\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decrease_epsilon_factor = decrease_epsilon_factor # larger -> more exploration\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def get_action(self, state, epsilon=None):   \n",
    "        if epsilon is None: \n",
    "            epsilon = self.epsilon\n",
    "                \n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            Q_values = self.get_q(state)   \n",
    "            return np.argmax(Q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "\n",
    "        # add data to replay buffer\n",
    "        \n",
    "        self.buffer.push(torch.tensor(state, dtype = torch.float32).unsqueeze(0), \n",
    "                           torch.tensor([[action]], dtype=torch.int64), \n",
    "                           torch.tensor([reward] , dtype = torch.float32), \n",
    "                           torch.tensor([terminated], dtype=torch.int64), \n",
    "                           torch.tensor(next_state , dtype = torch.float32).unsqueeze(0),\n",
    "                        )\n",
    "        \n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return np.inf\n",
    "\n",
    "        # get batch\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        # Compute loss - TO BE IMPLEMENTED!\n",
    "        # Hint: use the gather method from torch.\n",
    "\n",
    "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = tuple(\n",
    "            [torch.cat(data) for data in zip(*transitions)]\n",
    "        )\n",
    "\n",
    "        values  = self.q_net.forward(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute the ideal Q values\n",
    "        with torch.no_grad():\n",
    "            next_state_values = (1 - terminated_batch) * self.target_net(next_state_batch).max(1)[0]\n",
    "            targets = next_state_values * self.gamma + reward_batch\n",
    "\n",
    "        loss = self.loss_function(values, targets.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_value_(self.q_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if not((self.n_steps+1) % self.update_target_every): \n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "        self.decrease_epsilon()\n",
    "            \n",
    "        self.n_steps += 1\n",
    "        if terminated: \n",
    "            self.n_eps += 1\n",
    "\n",
    "        return loss.detach().numpy()\n",
    "    \n",
    "    def get_q(self, state):\n",
    "        \"\"\"\n",
    "        Compute Q function for a states\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.q_net.forward(state_tensor) # shape (1,  n_actions)\n",
    "        return output.numpy()[0]  # shape  (n_actions)\n",
    "    \n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
    "                        np.exp(-1. * self.n_eps / self.decrease_epsilon_factor ) )\n",
    "    \n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "        \n",
    "        obs_size = 448\n",
    "        n_actions = self.action_space.n\n",
    "        \n",
    "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
    "        self.q_net =  Net(obs_size, hidden_size, n_actions)\n",
    "        self.target_net = Net(obs_size, hidden_size, n_actions)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.q_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.n_steps = 0\n",
    "        self.n_eps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, *args):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, *data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env, n_sim=5):\n",
    "    \"\"\"\n",
    "    ** Solution **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, 0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length 19.53083206552987\n"
     ]
    }
   ],
   "source": [
    "import highway_env\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "with open(\"config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "    \n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n",
    "env.unwrapped.config.update(config)\n",
    "agent = RandomAgent(env.observation_space, env.action_space)\n",
    "\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, 0)\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')\n",
    "    \n",
    "run_one_episode(env, agent, display=True)\n",
    "#print(f'Average over 5 runs : {np.mean(eval_agent(agent, env))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(5)\n",
      "Box(-inf, inf, (5, 5), float32)\n",
      "episode = 10 , reward =  4.678540522158869\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m losses\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Evaluate the final policy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[88], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, N_episodes, eval_every, reward_threshold)\u001b[0m\n\u001b[1;32m     47\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m     49\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 50\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     53\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "Cell \u001b[0;32mIn[84], line 75\u001b[0m, in \u001b[0;36mDQN.update\u001b[0;34m(self, state, action, reward, terminated, next_state)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Optimize the model \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_value_(self.q_net.parameters(), 100)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"config.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "    \n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array')\n",
    "env.unwrapped.config.update(config)\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 128\n",
    "buffer_capacity = 10_000\n",
    "update_target_every = 32\n",
    "\n",
    "epsilon_start = 0.9\n",
    "decrease_epsilon_factor = 1000\n",
    "epsilon_min = 0.05\n",
    "\n",
    "learning_rate = 1e-2\n",
    "print(action_space)\n",
    "print(observation_space)\n",
    "arguments = (action_space,\n",
    "            observation_space,\n",
    "            gamma,\n",
    "            batch_size,\n",
    "            buffer_capacity,\n",
    "            update_target_every, \n",
    "            epsilon_start, \n",
    "            decrease_epsilon_factor, \n",
    "            epsilon_min,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "N_episodes = 300\n",
    "\n",
    "agent = DQN(*arguments)\n",
    "\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses\n",
    "\n",
    "    \n",
    "# Run the training loop\n",
    "losses = train(env, agent, N_episodes)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
